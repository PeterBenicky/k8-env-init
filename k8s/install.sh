# https://www.linuxtechi.com/install-kubernetes-cluster-on-debian/

sudo hostnamectl set-hostname "k8s-master.linuxtechi.local"      # Run on master node
sudo hostnamectl set-hostname "k8s-worker01.linuxtechi.local"    # Run on 1st worker node
sudo hostnamectl set-hostname "k8s-worker02.linuxtechi.local"    # Run on 2nd worker node

sudo nano /etc/hosts # run this on all workers
# the content (ip addresses need to be adjusted)
# 192.168.1.23   k8s-master.linuxtechi.local     k8s-master
# 192.168.1.24   k8s-worker01.linuxtechi.local   k8s-worker01
# 192.168.1.25   k8s-worker02.linuxtechi.local   k8s-worker02

sudo apt install ufw
sudo ufw --force enable

sudo ufw allow 6443/tcp
sudo ufw allow 2379/tcp
sudo ufw allow 2380/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 10251/tcp
sudo ufw allow 10252/tcp
sudo ufw allow 10255/tcp
sudo ufw reload

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf 
overlay 
br_netfilter
EOF

sudo modprobe overlay 
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1 
net.bridge.bridge-nf-call-ip6tables = 1 
EOF

sudo sysctl --system

sudo apt update
sudo apt -y install containerd

containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1

sudo nano /etc/containerd/config.toml # change ‘SystemdCgroup = false’ to ‘SystemdCgroup = true‘

sudo systemctl restart containerd
sudo systemctl enable containerd

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg  # Force Y

sudo apt update
sudo apt install kubelet kubeadm kubectl -y
sudo apt-mark hold kubelet kubeadm kubectl

sudo nano kubelet.yaml  # check the control plane endpoint, should match the hostname
sudo kubeadm init --config kubelet.yaml

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get nodes
kubectl cluster-info

# below is generated by <sudo kubeadm init --config kubelet.yaml>
kubeadm join server001:6443 --token aojifa.wxxyrph7yk67ocqc \
	--discovery-token-ca-cert-hash sha256:d8657b0e3800cd98588fae7f68d43c83a9e718d0f02c71b202523657c0228d04 \
	--control-plane






To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join server001:6443 --token aojifa.wxxyrph7yk67ocqc \
	--discovery-token-ca-cert-hash sha256:d8657b0e3800cd98588fae7f68d43c83a9e718d0f02c71b202523657c0228d04 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join server001:6443 --token aojifa.wxxyrph7yk67ocqc \
	--discovery-token-ca-cert-hash sha256:d8657b0e3800cd98588fae7f68d43c83a9e718d0f02c71b202523657c0228d04 